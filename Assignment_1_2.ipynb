{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MDLE - Exercise 2.1\n",
    "### Frequent itemsets and association rules - Similar items\n",
    "##### Authors: Pedro Duarte 97673, Pedro Monteiro 97484"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import random, itertools"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark Constants\n",
    "APP_NAME = 'assignment1ex2'\n",
    "MASTER = 'local[*]'\n",
    "\n",
    "# Similarity Constants\n",
    "MIN_HIGH_SIMILARITY = (.9, .85)\n",
    "MAX_LOW_SIMILARITY = (.05, .6)\n",
    "\n",
    "# Exercise Input Constants\n",
    "BANDS_ROWS_MAX_VALUE = 50"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration and Initialization of Spark\n",
    "\n",
    "- Parameters:\n",
    "    - `APP_NAME` (string): the name of the Spark application\n",
    "    - `MASTER` (string): the URL of the Spark master node\n",
    "<br></br>\n",
    "- Returns:\n",
    "    - `sc` (SparkContext): the Spark context for the given application and master\n",
    "    - `spark` (SparkSession): the Spark session for the given application and master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAppName(APP_NAME).setMaster(MASTER)\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "\n",
    "spark = SparkSession.builder.appName(APP_NAME).master(MASTER).config(\"spark.driver.memory\", \"15g\").getOrCreate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function `similar_probability`\n",
    "- calculate the probability that two things are similar, given their similarity score `s` and a reference value `r`\n",
    "\n",
    "function `not_similar_in_bands_probability` \n",
    "- calculate the probability that two things are not similar, given their similarity score `s`, a reference value `r`, and a number of comparison bands `b`\n",
    "\n",
    "function `similar_in_bands_probability`\n",
    "- calculate the probability that two things are similar, given their similarity score `s`, a reference value `r`, and a number of comparison bands `b`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lambda functions to abstract probability calcs\n",
    "similar_probability = lambda s, r: s**r \n",
    "not_similar_in_bands_probability = lambda s, r, b: (1 - similar_probability(s, r))**b\n",
    "similar_in_bands_probability = lambda s, r, b: 1 - not_similar_in_bands_probability(s, r, b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a list of valid pairs of values based on their similarity probabilities and threshold values <br><br>\n",
    "`parallelize` create pairs of values for range `BANDS_ROWS_MAX_VALUE` <br> <br>\n",
    "`filter` check if the probability of similarity between the minimum high similarity threshold (`MIN_HIGH_SIMILARITY[1]`) and the two values in the tuple (`v[0]` and `v[1]`) is greater than or equal to the minimum high similarity threshold value (`MIN_HIGH_SIMILARITY[0]`) <br> <br>\n",
    "`filter` check if the probability of similarity between the maximum low similarity threshold (`MAX_LOW_SIMILARITY[1]`) and the two values in the tuple (`v[0]` and `v[1]`) is less than the maximum low similarity threshold value (`MAX_LOW_SIMILARITY[0]`) <br> <br>\n",
    "`reduceByKey` reduce the RDD to only unique tuples with the minimum value <br> <br>\n",
    "`sortBy` sort RDD by the first element of each tuple <br> <br>\n",
    "`cache` RDD is cached in memory for faster access in the future <br> <br>\n",
    "`collect` retrieve all the elements in the RDD and store them in a list <br> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bands_rows_valid_pairs = sc.parallelize([(r, b) for r in range(BANDS_ROWS_MAX_VALUE) for b in range(BANDS_ROWS_MAX_VALUE)]) \\\n",
    "    .filter(lambda v: similar_in_bands_probability(MIN_HIGH_SIMILARITY[1], v[0], v[1]) >= MIN_HIGH_SIMILARITY[0]) \\\n",
    "    .filter(lambda v: similar_in_bands_probability(MAX_LOW_SIMILARITY[1], v[0], v[1]) < MAX_LOW_SIMILARITY[0]) \\\n",
    "    .reduceByKey(min) \\\n",
    "    .sortBy(lambda v: v[0]) \\\n",
    "    .cache()\n",
    "\n",
    "bands_rows_valid_pairs.collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create rows and bands from previous step. <br>\n",
    "Tested multiple combinations of rows and bands and the best was choosed.\n",
    "\n",
    "\n",
    "N_FUNCTIONS = number of rows * number of bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r, b = (13, 18)\n",
    "N_FUNCTIONS = r*b\n",
    "\n",
    "N_FUNCTIONS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Constants\n",
    "TWEET_ID_COLUMN = 'tweet_id'\n",
    "URL_COLUMN = 'url'\n",
    "TEXT_COLUMN = 'text'\n",
    "\n",
    "# Algorithm Constants\n",
    "MAX_SHINGLE_SIZE = 5\n",
    "\n",
    "# Input Constants\n",
    "INPUT_FILE = 'covid_news_small.json.bz2'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading and Parsing Data from CSV File\n",
    "\n",
    "- Parameters:\n",
    "    - `INPUT_FILE` (string): the path to the input CSV file\n",
    "<br></br>\n",
    "- Returns:\n",
    "    - `ds` (DataFrame): the parsed data as a Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = spark.read.json(INPUT_FILE)\n",
    "ds.schema"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate shingles from the tweets text and store them in a RDD, with duplicates removed and shingles sorted for each tweet. <br><br>\n",
    "`rdd` function is called to get the RDD representation of the dataframe <br> <br>\n",
    "`map` map each row of the RDD to a tuple of two values - tweet id and tweet text <br> <br>\n",
    "`filter` remove any tweets that have an empty text <br> <br>\n",
    "`mapValues` split the tweet text into words, create shingles from the words, and store the shingles in a list. If a word is smaller than the maximum shingle size, the word is added to the list directly. Otherwise, shingles are created from the word and added to the list. This is done for each tweet <br> <br>\n",
    "`mapValues` sort the shingles in each tweet's list <br> <br> \n",
    "`mapValues` convert each tweet's list of shingles into a set of shingles to remove duplicates <br><br>\n",
    "`cache` cached for faster access <br><br>\n",
    "`count` return the number of elements in the RDD <br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shingles = ds.rdd \\\n",
    "  .map(lambda v: (v[TWEET_ID_COLUMN], v[TEXT_COLUMN].casefold())) \\\n",
    "  .filter(lambda v: len(v[1])) \\\n",
    "  .mapValues(lambda v: [shingle for word in v.split() for shingle in ({word[i:i+MAX_SHINGLE_SIZE] for i in range(len(word) - MAX_SHINGLE_SIZE + 1)} if len(word) > MAX_SHINGLE_SIZE else [word])]) \\\n",
    "  .mapValues(sorted) \\\n",
    "  .mapValues(set) \\\n",
    "  .cache()\n",
    "\n",
    "shingles.count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create MinHashing Function Using Universal Hash <br>\n",
    "a,b - random numbers <br>\n",
    "24862048 - prime number > total shingles <br>\n",
    "len(shingles) - total shingles <br>\n",
    "\n",
    "Return:\n",
    "- returns a min hash for each function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_hashes(shingles, functions):\n",
    "    return [min([((a*shingle + b)%24862048)%len(shingles) for shingle in shingles]) for a, b in functions]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate random values to run previous function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [(random.randint(0, 100), random.randint(0, 100)) for _ in range(N_FUNCTIONS)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function `calc_band_hash` \n",
    "- take a row as an argument and return a list of tuples. Each tuple contains a hash of a shingle group (band) and its corresponding row ID.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_band_hash(row):\n",
    "    return [((hash(row[1][i*r:(i+1)*r]), i), row[0]) for i in range(b)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute all pairs of similar tweets by hashing each band of shingles for each tweet and comparing them, returning a distinct list of pairs of tweets that share at least one band hash and band index <br><br>\n",
    "`shingles` RDD is mapped to replace each tweet's set of shingles with a list of hash values for those shingles <br><br>\n",
    "`mapValues` compute the hash values for each band of shingles using the provided `build_hashes` function and a set of hash functions (`functions`) <br><br>\n",
    "`flatMap` apply the `calc_band_hash` function to each tweet's hash list and produce a list of tuples of the form `((band hash, band index), row ID)` <br><br>\n",
    "`groupByKey` group the tuples by their band hash and band index <br><br>\n",
    "`flatMap` produce pairs of tweets that share a band hash and band index <br><br>\n",
    "`distinct` remove any duplicate pairs of tweets <br><br>\n",
    "`collect` return a list of all distinct pairs of tweets that share at least one band hash and band index <br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_pairs = shingles \\\n",
    "  .mapValues(lambda v: [hash(s) for s in v]) \\\n",
    "  .mapValues(lambda v: build_hashes(v, functions)) \\\n",
    "  .flatMap(lambda v: calc_band_hash((v[0], tuple(v[1])))) \\\n",
    "  .groupByKey() \\\n",
    "  .flatMap(lambda v: itertools.combinations(v[1], 2)) \\\n",
    "  .distinct()\n",
    "  \n",
    "similar_pairs.collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dictionary of candidate pairs where each key represents a tweet and its value is a set of tweets that are similar to it based on the output of the previous code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_pairs = similar_pairs.flatMap(lambda v: [v, v[::-1]]).groupByKey().mapValues(set).collectAsMap()\n",
    "shingles = shingles.collectAsMap()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function `find_similar`\n",
    "- find similar pairs from the candidate pairs\n",
    "\n",
    "function `calc_similar`\n",
    "- calculate Jaccard similarity between an article and its candidates pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar(article_id: str, sim_treshold: float):\n",
    "    return [pair for pair in candidate_pairs[article_id] if len([None for s in shingles[article_id] if s in shingles[pair]])/len(shingles[article_id]) > sim_treshold]\n",
    "        \n",
    "def calc_similar(article_id: str):\n",
    "    return [len([None for s in shingles[article_id] if s in shingles[pair]])/len(shingles[article_id]) for pair in candidate_pairs[article_id]]\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of running the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_similar('1346893198283694085', .85)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2.3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parallelized the first 100 shingle keys (dataset sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = sc.parallelize(list(shingles.keys())[:100])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate false positive rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_positive_rate = subset \\\n",
    "  .filter(lambda v: v in candidate_pairs.keys()) \\\n",
    "  .map(lambda k: len([None for v in candidate_pairs[k] if v not in find_similar(k, .85)]) / len(candidate_pairs[k])) \\\n",
    "  .reduce(lambda v1, v2: v1 + v2) / len(shingles.keys()) * 100\n",
    "\n",
    "false_positive_rate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to calculate the Jaccard Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = lambda v1, v2: len([v for v in v1 if v in v2])/len(v1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate false negative rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_negative_percentages = subset.cartesian(subset) \\\n",
    "  .filter(lambda v: v[0] > v[1]) \\\n",
    "  .filter(lambda v: similarity(shingles[v[0]], shingles[v[1]]) > .85) \\\n",
    "  .flatMap(lambda v: [v, v[::-1]]) \\\n",
    "  .groupByKey() \\\n",
    "  .map(lambda v: (v[0], len([None for p in v[1] if p not in candidate_pairs[v[0]]]) / len(v[1]) if v[0] in candidate_pairs else 1)) \\\n",
    "  .groupByKey() \\\n",
    "  .mapValues(lambda v: sum(v)/len(v)) \\\n",
    "  .map(lambda v: v[1]) \\\n",
    "  .collect()\n",
    "\n",
    "false_negative_rate = sum(false_negative_percentages)/len(false_negative_percentages)\n",
    "false_negative_rate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
