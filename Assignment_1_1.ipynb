{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MDLE - Exercise 1.1\n",
    "### Frequent itemsets and association rules - Similar items\n",
    "##### Authors: Pedro Duarte 97673, Pedro Monteiro 97484"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from operator import add"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark Constants\n",
    "APP_NAME = 'assignment1'\n",
    "MASTER = 'local[*]'\n",
    "\n",
    "# Column Constants\n",
    "PATIENT_COLUMN = \"PATIENT\"\n",
    "CODE_COLUMN = \"CODE\"\n",
    "\n",
    "CONDITIONS_COLUMN = \"CONDITIONS\"\n",
    "\n",
    "# Input Constants\n",
    "INPUT_FILE = 'conditions.csv'\n",
    "SUPPORT_THRESHOLD = 1000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define combinations and frequent_combinations functions \n",
    "\n",
    "Function: combinations(elems, size, base=[])\n",
    "- generate combinations of elements from a given list `elems` of integers\n",
    "\n",
    "Function: frequent_combinations(elems, size, combinations, base=[])\n",
    "- generates all frequent combinations of size `size` from the list `elems` that appear in the list of previously generated combinations `combinations`\n",
    "- `base` parameter can be used to provide a starting list of combinations to build upon\n",
    "- if `base` is not provided, the function will use all single-element combinations from `elems`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combinations(elems, size, base=[]):\n",
    "    if len(elems) == 0 or size > len(elems): return []\n",
    "    if len(base) == 0: base = [(elem,) for elem in elems]\n",
    "    if len(base[0]) == size: return base\n",
    "    \n",
    "    base = [\n",
    "        base_comb + (elem,) \n",
    "        for base_comb in base\n",
    "        for elem in elems\n",
    "        if elem > base_comb[-1]\n",
    "    ]\n",
    "    \n",
    "    if len(base) == 0: return []\n",
    "\n",
    "    return combinations(elems, size, base)\n",
    "\n",
    "\n",
    "def frequent_combinations(elems, size, combinations, base=[]):\n",
    "    if len(elems) == 0 or size > len(elems): return []\n",
    "    if len(base) == 0: base = [(elem,) for elem in elems]\n",
    "    if len(base[0]) == size: return base\n",
    "    \n",
    "    base = [\n",
    "        base_comb + (elem,) \n",
    "        for base_comb in base\n",
    "        for elem in elems\n",
    "        if elem > base_comb[-1]\n",
    "        if len(base_comb) - 1 < size or (base_comb in combinations and base_comb[1:] + (elem,) in combinations)\n",
    "    ]\n",
    "\n",
    "    if len(base) == 0: return []\n",
    "\n",
    "    return frequent_combinations(elems, size, combinations, base)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration and Initialization of Spark\n",
    "\n",
    "- Parameters:\n",
    "    - `APP_NAME` (string): the name of the Spark application\n",
    "    - `MASTER` (string): the URL of the Spark master node\n",
    "<br></br>\n",
    "- Returns:\n",
    "    - `sc` (SparkContext): the Spark context for the given application and master\n",
    "    - `spark` (SparkSession): the Spark session for the given application and master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAppName(APP_NAME).setMaster(MASTER)\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "\n",
    "spark = SparkSession.builder.appName(APP_NAME).master(MASTER).getOrCreate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading and Parsing Data from CSV File\n",
    "\n",
    "- Parameters:\n",
    "    - `INPUT_FILE` (string): the path to the input CSV file\n",
    "<br></br>\n",
    "- Returns:\n",
    "    - `ds` (DataFrame): the parsed data as a Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StructType([StructField('START', TimestampType(), True), StructField('STOP', TimestampType(), True), StructField('PATIENT', StringType(), True), StructField('ENCOUNTER', StringType(), True), StructField('CODE', LongType(), True), StructField('DESCRIPTION', StringType(), True)])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = spark.read.csv(INPUT_FILE, header=True, inferSchema=True)\n",
    "ds.schema"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting and Aggregating Patient Conditions\n",
    "\n",
    "1. Maps each row to a patient ID and a set containing their conditions.\n",
    "2. Reduces the data by patient ID, combining the sets of condition codes for each patient into a single set.\n",
    "3. Sorts the condition codes for each patient in ascending order.\n",
    "4. Collects the resulting data into a list of tuples, where each tuple contains a patient ID and their sorted set of condition codes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('3826037f-19e0-4c7b-98e5-4e9578472f67', [24079001, 55822004, 65966004, 162864005]), ('8e763f75-614b-4ef7-aa86-ce459dd3142e', [10509002, 70704007, 128613002, 195662009, 703151001]), ('b7ee9259-1f13-412a-830c-a53684b82cc3', [15777000, 40055000, 49436004, 59621000, 65275009, 68496003, 92691004, 126906006, 162573006, 162864005, 235919008, 254632001, 271737000, 370143000, 403190006, 444814009, 67811000119102]), ('2593819d-f0ff-470b-95da-656e8340255c', [10509002, 19169002, 35999006, 72892002, 195662009, 198992004, 232353008, 398254007, 444814009]), ('0800eff6-6e91-4014-8349-52023f4975b7', [10509002, 15777000, 19169002, 59621000, 64859006, 68496003, 162864005, 444814009])]\n"
     ]
    }
   ],
   "source": [
    "patient_conditions = ds.rdd \\\n",
    "  .map(lambda v: (v[PATIENT_COLUMN], {v[CODE_COLUMN]})) \\\n",
    "  .reduceByKey(lambda v1, v2: v1.union(v2)) \\\n",
    "  .mapValues(sorted) \\\n",
    "  .collect()\n",
    "\n",
    "print(patient_conditions[:5])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating Frequent Itemsets\n",
    "1. Converts the patient condition into a set of itemsets by flattening the list of conditions for each patient.\n",
    "2. Reduces the data by itemset, counting the number of occurrences of each itemset.\n",
    "3. Filters itemsets that do not meet the minimum support threshold.\n",
    "4. Caches the resulting RDD for faster access in subsequent iterations.\n",
    "5. Collects the frequent itemsets into a Python dictionary.\n",
    "6. Computes the number of frequent itemsets generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/19 12:20:42 WARN TaskSetManager: Stage 4 contains a task of very large size (7229 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "131"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "base_results = sc.parallelize(patient_conditions) \\\n",
    "  .flatMap(lambda v: [(c, 1) for c in v[1]]) \\\n",
    "  .reduceByKey(add) \\\n",
    "  .filter(lambda x: x[1] >= SUPPORT_THRESHOLD) \\\n",
    "  .cache() # Cache the resulting RDD for faster access in subsequent iterations\n",
    "\n",
    "base_elements = base_results.map(lambda v: v[0]).collect()\n",
    "base_results = base_results.collectAsMap()\n",
    "\n",
    "len(base_elements) # 131"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_patient_conditions = sc.parallelize(patient_conditions) \\\n",
    "  .map(lambda v: [c for c in v[1] if c in base_elements]) \\\n",
    "  .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/19 12:20:46 WARN TaskSetManager: Stage 8 contains a task of very large size (7229 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2940"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs_results = filtered_patient_conditions \\\n",
    "  .flatMap(lambda v: [(c, 1) for c in combinations(v, 2)]) \\\n",
    "  .reduceByKey(add) \\\n",
    "  .filter(lambda x: x[1] >= SUPPORT_THRESHOLD) \\\n",
    "  .cache()\n",
    "\n",
    "frequent_pairs = pairs_results.map(lambda v: v[0]).collect()\n",
    "pairs_results = pairs_results.collectAsMap()\n",
    "\n",
    "len(frequent_pairs) # 2940"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/19 12:20:54 WARN TaskSetManager: Stage 12 contains a task of very large size (7229 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13395"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triples_results = filtered_patient_conditions \\\n",
    "  .flatMap(lambda v: [(c, 1) for c in frequent_combinations(v, 3, frequent_pairs)]) \\\n",
    "  .reduceByKey(add) \\\n",
    "  .filter(lambda x: x[1] >= SUPPORT_THRESHOLD) \\\n",
    "  .cache()\n",
    "\n",
    "frequent_triples = triples_results.map(lambda v: v[0]).collect()\n",
    "triples_results = triples_results.collectAsMap()\n",
    "\n",
    "len(frequent_triples) # 13395"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Constants\n",
    "STD_LIFT_THRESHOLD = .2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_line(v, get_combination_support, union_support_results):\n",
    "    n = len(patient_conditions)\n",
    "\n",
    "    combination_support = get_combination_support(v[:-1])\n",
    "    element_support = base_results[v[-1]]\n",
    "    union_support = union_support_results[tuple(sorted(v))]\n",
    "\n",
    "    combination_probability = combination_support/n\n",
    "    elem_probability = element_support/n\n",
    "    \n",
    "    confidence = union_support/combination_support\n",
    "    interest = confidence - elem_probability\n",
    "    lift = confidence/elem_probability\n",
    "\n",
    "    z = max(combination_probability+elem_probability-1, 1/n)/(combination_probability*elem_probability)\n",
    "    std_lift = (lift - z)/(1/max(combination_probability, elem_probability) - z)\n",
    "\n",
    "    return (v, std_lift, lift, confidence, interest)\n",
    "\n",
    "create_pair_line = lambda v: create_line(v, lambda c: base_results[c[0]], pairs_results)\n",
    "create_triple_line = lambda v: create_line(v, lambda c: pairs_results[c], triples_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2418"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs_rules = sc.parallelize(frequent_pairs) \\\n",
    "  .flatMap(lambda v: [create_pair_line(v), create_pair_line(v[::-1])]) \\\n",
    "  .filter(lambda v: v[1] > STD_LIFT_THRESHOLD) \\\n",
    "  .sortBy(lambda v: v[1]) \\\n",
    "  .collect()\n",
    "\n",
    "len(pairs_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "23247"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triples_rules = sc.parallelize(frequent_triples) \\\n",
    "  .flatMap(lambda v: [create_triple_line(v), create_triple_line(v[1:] + v[:1]), create_triple_line(v[:1] + v[2:] + v[1:2])]) \\\n",
    "  .filter(lambda v: v[1] > STD_LIFT_THRESHOLD) \\\n",
    "  .sortBy(lambda v: v[1]) \\\n",
    "  .collect()\n",
    "\n",
    "len(triples_rules)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
