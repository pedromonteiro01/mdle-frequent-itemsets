{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MDLE - Exercise 1.1\n",
    "### Frequent itemsets and association rules - Similar items\n",
    "##### Authors: Pedro Duarte 97673, Pedro Monteiro 97484"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from operator import add"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark Constants\n",
    "APP_NAME = 'assignment1'\n",
    "MASTER = 'local[*]'\n",
    "\n",
    "# Column Constants\n",
    "PATIENT_COLUMN = \"PATIENT\"\n",
    "CODE_COLUMN = \"CODE\"\n",
    "\n",
    "CONDITIONS_COLUMN = \"CONDITIONS\"\n",
    "\n",
    "# Input Constants\n",
    "INPUT_FILE = 'conditions.csv.gz'\n",
    "SUPPORT_THRESHOLD = 1000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define combinations and frequent_combinations functions \n",
    "\n",
    "Function: combinations(elems, size, base=[])\n",
    "- generate combinations of elements from a given list `elems` of integers\n",
    "\n",
    "Function: frequent_combinations(elems, size, combinations, base=[])\n",
    "- generates all frequent combinations of size `size` from the list `elems` that appear in the list of previously generated combinations `combinations`\n",
    "- `base` parameter can be used to provide a starting list of combinations to build upon\n",
    "- if `base` is not provided, the function will use all single-element combinations from `elems`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our combinations function\n",
    "def combinations(elems, size, base=[]):\n",
    "    if len(elems) == 0 or size > len(elems): return []\n",
    "    if len(base) == 0: base = [(elem,) for elem in elems]\n",
    "    if len(base[0]) == size: return base\n",
    "    \n",
    "    base = [\n",
    "        base_comb + (elem,) \n",
    "        for base_comb in base\n",
    "        for elem in elems\n",
    "        if elem > base_comb[-1]\n",
    "    ]\n",
    "    \n",
    "    if len(base) == 0: return []\n",
    "\n",
    "    return combinations(elems, size, base)\n",
    "\n",
    "\n",
    "def frequent_combinations(elems, size, combinations, base=[]):\n",
    "    if len(elems) == 0 or size > len(elems): return []\n",
    "    if len(base) == 0: base = [(elem,) for elem in elems]\n",
    "    if len(base[0]) == size: return base\n",
    "    \n",
    "    base = [\n",
    "        base_comb + (elem,) \n",
    "        for base_comb in base\n",
    "        for elem in elems\n",
    "        if elem > base_comb[-1]\n",
    "        if len(base_comb) - 1 < size or (base_comb in combinations and base_comb[1:] + (elem,) in combinations)\n",
    "    ]\n",
    "\n",
    "    if len(base) == 0: return []\n",
    "\n",
    "    return frequent_combinations(elems, size, combinations, base)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration and Initialization of Spark\n",
    "\n",
    "- Parameters:\n",
    "    - `APP_NAME` (string): the name of the Spark application\n",
    "    - `MASTER` (string): the URL of the Spark master node\n",
    "<br></br>\n",
    "- Returns:\n",
    "    - `sc` (SparkContext): the Spark context for the given application and master\n",
    "    - `spark` (SparkSession): the Spark session for the given application and master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAppName(APP_NAME).setMaster(MASTER)\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "\n",
    "spark = SparkSession.builder.appName(APP_NAME).master(MASTER).getOrCreate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading and Parsing Data from CSV File\n",
    "\n",
    "- Parameters:\n",
    "    - `INPUT_FILE` (string): the path to the input CSV file\n",
    "<br></br>\n",
    "- Returns:\n",
    "    - `ds` (DataFrame): the parsed data as a Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = spark.read.csv(INPUT_FILE, header=True, inferSchema=True)\n",
    "ds.schema"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting and Aggregating Patient Conditions\n",
    "\n",
    "1. Maps each row to a patient ID and a set containing their conditions.\n",
    "2. Reduces the data by patient ID, combining the sets of condition codes for each patient into a single set.\n",
    "3. Sorts the condition codes for each patient in ascending order.\n",
    "4. Collects the resulting data into a list of tuples, where each tuple contains a patient ID and their sorted set of condition codes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_conditions = ds.rdd \\\n",
    "  .map(lambda v: (v[PATIENT_COLUMN], {v[CODE_COLUMN]})) \\\n",
    "  .reduceByKey(lambda v1, v2: v1.union(v2)) \\\n",
    "  .mapValues(sorted) \\\n",
    "  .collect()\n",
    "\n",
    "print(patient_conditions[:5])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating Frequent Itemsets\n",
    "1. Converts the patient condition into a set of itemsets by flattening the list of conditions for each patient.\n",
    "2. Reduces the data by itemset, counting the number of occurrences of each itemset.\n",
    "3. Filters itemsets that do not meet the minimum support threshold.\n",
    "4. Caches the resulting RDD for faster access in subsequent iterations.\n",
    "5. Collects the frequent itemsets into a Python dictionary.\n",
    "6. Computes the number of frequent itemsets generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "base_results = sc.parallelize(patient_conditions) \\\n",
    "  .flatMap(lambda v: [(c, 1) for c in v[1]]) \\\n",
    "  .reduceByKey(add) \\\n",
    "  .filter(lambda x: x[1] >= SUPPORT_THRESHOLD) \\\n",
    "  .cache() # Cache the resulting RDD for faster access in subsequent iterations\n",
    "\n",
    "base_elements = base_results.map(lambda v: v[0]).collect()\n",
    "base_results = base_results.collectAsMap()\n",
    "\n",
    "len(base_elements) # 131"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create patient conditions and remove elements that are not in the base_elements list\n",
    "\n",
    "`sc.parallelize()` method is used to create the RDD from the patient_conditions list. <br>\n",
    "`map()` method is used to apply the filtering transformation to each patient condition. <br>\n",
    "`cache()` method is used to cache the resulting filtered list in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_patient_conditions = sc.parallelize(patient_conditions) \\\n",
    "  .map(lambda v: [c for c in v[1] if c in base_elements]) \\\n",
    "  .cache()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute frequent pairs of elements from the `filtered_patient_conditions` RDD\n",
    "\n",
    "`flatMap()` is used to generate all pairs of elements in each patient condition using `combinations()` function\n",
    "\n",
    "`reduceByKey()` is used to aggregate the each pair counts across patient\n",
    "conditions.\n",
    "\n",
    "`filter()` is used to remove pairs that do not meet a certain of support threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_results = filtered_patient_conditions \\\n",
    "  .flatMap(lambda v: [(c, 1) for c in combinations(v, 2)]) \\\n",
    "  .reduceByKey(add) \\\n",
    "  .filter(lambda x: x[1] >= SUPPORT_THRESHOLD) \\\n",
    "  .cache()\n",
    "\n",
    "top_ten_pairs = pairs_results.sortBy(lambda x: -x[1]).toDF().head(10)\n",
    "frequent_pairs = pairs_results.map(lambda v: v[0]).collect()\n",
    "pairs_results = pairs_results.collectAsMap()\n",
    "\n",
    "#len(frequent_pairs) ==> 2940"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute frequent triples of elements from the `filtered_patient_conditions`\n",
    "\n",
    "`flatMap()` is used to generate all triples of elements in each patient condition using `frequent_combinations()` function\n",
    "- `frequent_combinations()` filters pairs that are not frequent based on the list of frequent pairs previous computed.\n",
    "\n",
    "`reduceByKey()` used to aggregate the counts of each triple \n",
    "\n",
    "`filter()` is used to remove triples that do not meet the minimum support threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triples_results = filtered_patient_conditions \\\n",
    "  .flatMap(lambda v: [(c, 1) for c in frequent_combinations(v, 3, frequent_pairs)]) \\\n",
    "  .reduceByKey(add) \\\n",
    "  .filter(lambda x: x[1] >= SUPPORT_THRESHOLD) \\\n",
    "  .cache()\n",
    "\n",
    "top_ten_triplets = triples_results.sortBy(lambda x: -x[1]).toDF().head(10)\n",
    "frequent_triples = triples_results.map(lambda v: v[0]).collect()\n",
    "triples_results = triples_results.collectAsMap()\n",
    "\n",
    "# len(frequent_triples) ==> 13395"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show top 10 pair results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_ten_pairs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show top 10 triplets results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_ten_triplets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1.2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Constants\n",
    "STD_LIFT_THRESHOLD = .2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze patient data and determine the relationship between various patient conditions <br>\n",
    "<br>\n",
    "function `create_line` that takes three arguments:\n",
    "- v: items list representing a combination of patient conditions\n",
    "- get_combination_support: a function that takes in a combination of patient conditions and returns the support of that combination\n",
    "- union_support_results: a dictionary containing the support of all possible combinations of patient conditions\n",
    "\n",
    "Returns:\n",
    "- tuple containing the combination of patient conditions (variable v), the standard lift, lift, confidence, and interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_line(v, get_combination_support, union_support_results):\n",
    "    n = len(patient_conditions)\n",
    "\n",
    "    combination_support = get_combination_support(v[:-1]) # calculate combination support of patient conditions\n",
    "    element_support = base_results[v[-1]] # support of the individual element in the combination\n",
    "    union_support = union_support_results[tuple(sorted(v))] # support of the union of all elements in the combination\n",
    "\n",
    "    combination_probability = combination_support/n\n",
    "    elem_probability = element_support/n\n",
    "    \n",
    "    # calculate confidence, interest and lift\n",
    "    confidence = union_support/combination_support\n",
    "    interest = confidence - elem_probability\n",
    "    lift = confidence/elem_probability\n",
    "\n",
    "    z = max(combination_probability+elem_probability-1, 1/n)/(combination_probability*elem_probability)\n",
    "    std_lift = (lift - z)/(1/max(combination_probability, elem_probability) - z) # calculate the standard lift\n",
    "\n",
    "    return (v, std_lift, lift, confidence, interest)\n",
    "\n",
    "# create pair and triple rules\n",
    "create_pair_line = lambda v: create_line(v, lambda c: base_results[c[0]], pairs_results)\n",
    "create_triple_line = lambda v: create_line(v, lambda c: pairs_results[c], triples_results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create pair rules\n",
    "- Take a list of frequent pairs \n",
    "- Create new pairs of items by reversing the order of the pairs\n",
    "- Filter pairs that do not meet a certain criterion\n",
    "- Sort the remaining pairs\n",
    "\n",
    "`sc.parallelize(frequent_pairs)` create a Spark RDD from the `frequent_pairs` list <br>\n",
    "`flatMap` used to transform the frequent_pairs list RDD into an RDD of pairs of items  <br>\n",
    "`filter` removes pairs where the second element of the pair (lift value) is less than a predefined `STD_LIFT_THRESHOLD` <br>\n",
    "`sortBy` sorts pairs in ascending order <br>\n",
    "`collect` return the results as a list of tuples<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_rules = sc.parallelize(frequent_pairs) \\\n",
    "  .flatMap(lambda v: [create_pair_line(v), create_pair_line(v[::-1])]) \\\n",
    "  .filter(lambda v: v[1] > STD_LIFT_THRESHOLD) \\\n",
    "  .sortBy(lambda v: v[1]) \\\n",
    "  .collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write results to `pair_rules.txt` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pair_rules.txt', 'w') as file:\n",
    "  file.write('({})->{:<30}{:<30}{:<30}{:<30}{:<30}\\n'\n",
    "             .format(\"X\", \"Y\", \"Standardised Lift\", \"Lift\", \"Confidence\", \"Interest\"))\n",
    "  for item in pairs_rules:\n",
    "    file.write('({:<})->{:<20}{:<30}{:<30}{:<30}{:<30}\\n'.format(\n",
    "    item[0][0], item[0][1], item[1], item[2], item[3], item[4]\n",
    "    ))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create triplet rules\n",
    "\n",
    "- Takes a list of frequent triples\n",
    "- Create new triples by shifting the elements and combining them in various ways\n",
    "- Filter triples that do not meet a certain criterion\n",
    "- Sort triples\n",
    "\n",
    "`sc.parallelize(frequent_triples):` create an RDD from the frequent_triples list <br>\n",
    "`flatMap` transform the RDD created from the `frequent_triples` list into an RDD of triples of items<br>\n",
    "`filter` remove triples where the second element (lift value) is less than a predefined `STD_LIFT_THRESHOLD` <br>\n",
    "`sortBy` sort the triples in ascending order based on the second element (lift value) <br>\n",
    "`collect` return data as a list of tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triples_rules = sc.parallelize(frequent_triples) \\\n",
    "  .flatMap(lambda v: [create_triple_line(v), create_triple_line(v[1:] + v[:1]), create_triple_line(v[:1] + v[2:] + v[1:2])]) \\\n",
    "  .filter(lambda v: v[1] > STD_LIFT_THRESHOLD) \\\n",
    "  .sortBy(lambda v: v[1]) \\\n",
    "  .collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write results to `triplets_rules.txt` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('triplet_rules.txt', 'w') as file:\n",
    "  file.write('({},{})->{:<30}{:<30}{:<30}{:<30}{:<30}\\n'\n",
    "             .format(\"X\", \"Y\", \"Z\", \"Standardised Lift\", \"Lift\", \"Confidence\", \"Interest\"))\n",
    "  for item in triples_rules:\n",
    "    file.write('({},{})->{:<20}{:<30}{:<30}{:<30}{:<30}\\n'.format(\n",
    "    item[0][0], item[0][1], item[0][2], item[1], item[2], item[3], item[4]\n",
    "    ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
