{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from operator import add"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark Constants\n",
    "APP_NAME = 'assignment1'\n",
    "MASTER = 'local[*]'\n",
    "\n",
    "# Column Constants\n",
    "PATIENT_COLUMN = \"PATIENT\"\n",
    "CODE_COLUMN = \"CODE\"\n",
    "\n",
    "CONDITIONS_COLUMN = \"CONDITIONS\"\n",
    "\n",
    "# Input Constants\n",
    "INPUT_FILE = 'conditions.csv'\n",
    "SUPPORT_THRESHOLD = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combinations(elems, size, base=[]):\n",
    "    if len(elems) == 0 or size > len(elems): return []\n",
    "    if len(base) == 0: base = [(elem,) for elem in elems]\n",
    "    if len(base[0]) == size: return base\n",
    "    \n",
    "    base = [\n",
    "        base_comb + (elem,) \n",
    "        for base_comb in base\n",
    "        for elem in elems\n",
    "        if elem > base_comb[-1]\n",
    "    ]\n",
    "    \n",
    "    if len(base) == 0: return []\n",
    "\n",
    "    return combinations(elems, size, base)\n",
    "\n",
    "\n",
    "def frequent_combinations(elems, size, combinations, base=[]):\n",
    "    if len(elems) == 0 or size > len(elems): return []\n",
    "    if len(base) == 0: base = [(elem,) for elem in elems]\n",
    "    if len(base[0]) == size: return base\n",
    "    \n",
    "    base = [\n",
    "        base_comb + (elem,) \n",
    "        for base_comb in base\n",
    "        for elem in elems\n",
    "        if elem > base_comb[-1]\n",
    "        if len(base_comb) - 1 < size or (base_comb in combinations and base_comb[1:] + (elem,) in combinations)\n",
    "    ]\n",
    "\n",
    "    if len(base) == 0: return []\n",
    "\n",
    "    return frequent_combinations(elems, size, combinations, base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/18 16:21:29 WARN Utils: Your hostname, pedro-duarte resolves to a loopback address: 127.0.1.1; using 192.168.0.103 instead (on interface wlp2s0)\n",
      "23/03/18 16:21:29 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/18 16:21:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/03/18 16:21:33 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/03/18 16:21:33 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "conf = SparkConf().setAppName(APP_NAME).setMaster(MASTER)\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "\n",
    "spark = SparkSession.builder.appName(APP_NAME).master(MASTER).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StructType([StructField('START', TimestampType(), True), StructField('STOP', TimestampType(), True), StructField('PATIENT', StringType(), True), StructField('ENCOUNTER', StringType(), True), StructField('CODE', LongType(), True), StructField('DESCRIPTION', StringType(), True)])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = spark.read.csv(INPUT_FILE, header=True, inferSchema=True)\n",
    "ds.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread \"serve RDD 42\" java.net.SocketTimeoutException: Accept timed out\n",
      "\tat java.base/java.net.PlainSocketImpl.socketAccept(Native Method)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)\n",
      "\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:533)\n",
      "\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:64)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('28a3cdb7-1db1-4148-8280-8a4e5b4f99e0', [19169002, 72892002, 156073000, 284551006]), ('3826037f-19e0-4c7b-98e5-4e9578472f67', [24079001, 55822004, 65966004, 162864005]), ('e32e0069-2d3f-4b7b-b420-3269c94723ad', [16114001, 162864005, 195662009]), ('887ad9bb-bd72-44cf-8e5e-8aff7fbdeed4', [40275004, 44465007, 72892002, 195662009, 444814009]), ('8e763f75-614b-4ef7-aa86-ce459dd3142e', [10509002, 70704007, 128613002, 195662009, 703151001])]\n"
     ]
    }
   ],
   "source": [
    "patient_conditions = ds.rdd \\\n",
    "  .map(lambda v: (v[PATIENT_COLUMN], {v[CODE_COLUMN]})) \\\n",
    "  .reduceByKey(lambda v1, v2: v1.union(v2)) \\\n",
    "  .mapValues(sorted) \\\n",
    "  .collect()\n",
    "\n",
    "print(patient_conditions[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/18 16:54:17 WARN TaskSetManager: Stage 47 contains a task of very large size (21666 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "131"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "base_results = sc.parallelize(patient_conditions) \\\n",
    "  .flatMap(lambda v: [(c, 1) for c in v[1]]) \\\n",
    "  .reduceByKey(add) \\\n",
    "  .filter(lambda x: x[1] >= SUPPORT_THRESHOLD) \\\n",
    "  .cache()\n",
    "\n",
    "base_elements = base_results.map(lambda v: v[0]).collect()\n",
    "base_results = base_results.collectAsMap()\n",
    "\n",
    "len(base_elements) # 131"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_patient_conditions = sc.parallelize(patient_conditions) \\\n",
    "  .map(lambda v: [c for c in v[1] if c in base_elements]) \\\n",
    "  .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/18 16:36:17 WARN TaskSetManager: Stage 31 contains a task of very large size (21666 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2940"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs_results = filtered_patient_conditions \\\n",
    "  .flatMap(lambda v: [(c, 1) for c in combinations(v, 2)]) \\\n",
    "  .reduceByKey(add) \\\n",
    "  .filter(lambda x: x[1] >= SUPPORT_THRESHOLD) \\\n",
    "  .cache()\n",
    "\n",
    "frequent_pairs = pairs_results.map(lambda v: v[0]).collect()\n",
    "pairs_results = pairs_results.collectAsMap()\n",
    "\n",
    "len(frequent_pairs) # 2940"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/18 16:40:23 WARN TaskSetManager: Stage 43 contains a task of very large size (21666 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13395"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triples_results = filtered_patient_conditions \\\n",
    "  .flatMap(lambda v: [(c, 1) for c in frequent_combinations(v, 3, frequent_pairs)]) \\\n",
    "  .reduceByKey(add) \\\n",
    "  .filter(lambda x: x[1] >= SUPPORT_THRESHOLD) \\\n",
    "  .cache()\n",
    "\n",
    "frequent_triples = triples_results.map(lambda v: v[0]).collect()\n",
    "triples_results = triples_results.collectAsMap()\n",
    "\n",
    "len(frequent_triples) # 13395"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Constants\n",
    "STD_LIFT_THRESHOLD = .2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_line(v, get_combination_support, union_support_results):\n",
    "    n = len(patient_conditions)\n",
    "\n",
    "    combination_support = get_combination_support(v[:-1])\n",
    "    element_support = base_results[v[-1]]\n",
    "    union_support = union_support_results[tuple(sorted(v))]\n",
    "\n",
    "    combination_probability = combination_support/n\n",
    "    elem_probability = element_support/n\n",
    "    \n",
    "    confidence = union_support/combination_support\n",
    "    interest = confidence - elem_probability\n",
    "    lift = confidence/elem_probability\n",
    "\n",
    "    z = max(combination_probability+elem_probability-1, 1/n)/(combination_probability*elem_probability)\n",
    "    std_lift = (lift - z)/(1/max(combination_probability, elem_probability) - z)\n",
    "\n",
    "    return (v, std_lift, lift, confidence, interest)\n",
    "\n",
    "create_pair_line = lambda v: create_line(v, lambda c: base_results[c[0]], pairs_results)\n",
    "create_triple_line = lambda v: create_line(v, lambda c: pairs_results[c], triples_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2418"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs_rules = sc.parallelize(frequent_pairs) \\\n",
    "  .flatMap(lambda v: [create_pair_line(v), create_pair_line(v[::-1])]) \\\n",
    "  .filter(lambda v: v[1] > STD_LIFT_THRESHOLD) \\\n",
    "  .sortBy(lambda v: v[1]) \\\n",
    "  .collect()\n",
    "\n",
    "len(pairs_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "23247"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triples_rules = sc.parallelize(frequent_triples) \\\n",
    "  .flatMap(lambda v: [create_triple_line(v), create_triple_line(v[1:] + v[:1]), create_triple_line(v[:1] + v[2:] + v[1:2])]) \\\n",
    "  .filter(lambda v: v[1] > STD_LIFT_THRESHOLD) \\\n",
    "  .sortBy(lambda v: v[1]) \\\n",
    "  .collect()\n",
    "\n",
    "len(triples_rules)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
